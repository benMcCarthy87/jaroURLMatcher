{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "URLMatcher.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPP0r0ifa6iZX+rZzdUHC1Y"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn4OCldKhVbx"
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "\n",
        "'''\n",
        "If the below is True, matches scored 'above average' will be removed from the list to match with, this will speed up the process of matching remaining URLs\n",
        "This is a bad option if you have few URL changes, or if you have consolidated URLs so there will be multiple matches to your new URL.\n",
        "'''\n",
        "remove_urls = True\n",
        "\n",
        "'''\n",
        "If the below is True, if a match scored above average, it will return this and not check the rest of the list. If this is false URLs the above remove_urls\n",
        "option will become irrelevant - URLs won't be removed when found.\n",
        "'''\n",
        "\n",
        "match_if_above_ave = True"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U7q02sDkxZR"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhAxI7Jek2OG"
      },
      "source": [
        "df = pd.read_csv(io.BytesIO(uploaded['urls.csv']))\n",
        "print(df[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ3d5dkT5j64"
      },
      "source": [
        "from math import floor, ceil \n",
        "def jaro_distance(s1, s2): \n",
        "    if (s1 == s2): \n",
        "        return 1.0\n",
        "    len1 = len(s1) \n",
        "    len2 = len(s2) \n",
        "    max_dist = floor(max(len1, len2) / 2) - 1\n",
        "    match = 0 \n",
        "    hash_s1 = [0] * len(s1) \n",
        "    hash_s2 = [0] * len(s2) \n",
        "    for i in range(len1): \n",
        "        for j in range(max(0, i - max_dist),  \n",
        "                       min(len2, i + max_dist + 1)): \n",
        "            if (s1[i] == s2[j] and hash_s2[j] == 0): \n",
        "                hash_s1[i] = 1\n",
        "                hash_s2[j] = 1\n",
        "                match += 1\n",
        "                break\n",
        "    if (match == 0): \n",
        "        return 0.0\n",
        "    t = 0\n",
        "    point = 0\n",
        "    for i in range(len1): \n",
        "        if (hash_s1[i]): \n",
        "            while (hash_s2[point] == 0): \n",
        "                point += 1\n",
        "            if (s1[i] != s2[point]): \n",
        "                point += 1\n",
        "                t += 1\n",
        "    t = t//2\n",
        "    return (match/ len1 + match / len2 + \n",
        "            (match - t + 1) / match)/ 3.0\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xlg9LU_fsj6i"
      },
      "source": [
        "i = 0\n",
        "all_distances = []\n",
        "hld = -100\n",
        "#finds the best match using Jaroâ€“Winkler distance.\n",
        "def get_url(urlO):\n",
        "    hld = -100\n",
        "    bm = 'NO URL'\n",
        "    for  urlN in new_urls:\n",
        "      ld = jaro_distance(str(urlO), str(urlN))\n",
        "      simil = ld\n",
        "      if ld > hld:\n",
        "        hld = ld\n",
        "        bm = urlN\n",
        "        if ld > np.mean(all_distances) and len(all_distances) > 10 and len(bm) > 3 and len(new_urls) > 100:\n",
        "          print('Found above average match')\n",
        "          if remove_urls == True:\n",
        "            new_urls.remove(bm)\n",
        "            print('removed ' + bm)\n",
        "            print(str(len(new_urls)) + ' left in new URLs')\n",
        "          return bm\n",
        "    all_distances.append(hld)\n",
        "    return bm\n",
        "     "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CiWIbLQrfKT"
      },
      "source": [
        "matched_urls = []\n",
        "old_urls = df['old_URLs'].tolist()\n",
        "new_urls = df['new_URLs'].tolist()\n",
        "\n",
        "#Removing unchanged URLs\n",
        "\n",
        "old_out = []\n",
        "matched_out = []\n",
        "for old in old_urls:\n",
        "  if old in new_urls:\n",
        "      old_out.append(old)\n",
        "      old_urls.remove(old)\n",
        "      matched_out.append(old)\n",
        "      new_urls.remove(old)\n",
        "\n",
        "#Looping through old URLs, using the get_url function to find the bestmatch\n",
        "for urlO in old_urls:\n",
        "  bm = get_url(urlO)\n",
        "  matched_urls.append(bm)\n",
        "  print(str(urlO) + ' matched with ' + str(bm))\n",
        "  i += 1\n",
        "  print(i, end = '')\n",
        "  print(' of ', end = '')\n",
        "  print(len(old_urls))\n",
        "\n",
        "df_Results = pd.DataFrame(old_urls,columns=['old_urls'])\n",
        "df_Results.insert(1, 'matched_urls', matched_urls, True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoTzwYyu-i_e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT25rJ8gvS2K"
      },
      "source": [
        "df_Results.to_csv('URL_Migration_Results.csv')\n",
        "!cp URL_Migration_Results.csv \"drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}